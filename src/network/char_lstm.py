import torch
from torch import Tensor, nn

from settings import LstmModelSettings


class CharLSTM(nn.Module):
    """A character-level recurrent neural network based on LSTM layers for text classification.

    The model is composed of an LSTM layer, a dropout layer, an output layer, and a LogSoftmax activation
    function for compatibility with negative log likelihood loss.
    """

    def __init__(self, input_size: int, output_size: int, model_settings: LstmModelSettings) -> None:
        """Build neural network layers.

        :param input_size: The size of the input features.
        :param output_size: The number of output classes.
        :param model_settings: An object containing the settings for the model such as
            number of hidden units, number of layers, dropout probability, and others.
        """
        super().__init__()

        self.model_settings = model_settings
        self.input_size = input_size
        self.output_size = output_size

        # LSTM layer that processes character sequences
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=model_settings.n_hidden_units,
            num_layers=model_settings.n_layers,
            batch_first=True,
            bidirectional=False,
            dropout=model_settings.dropout_p,
        )
        self.dropout = nn.Dropout(p=model_settings.dropout_p)

        # Output layer that maps hidden states to class predictions
        self.output_layer = nn.Linear(model_settings.n_hidden_units, output_size)

        # LogSoftmax for NLL loss compatibility
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, input_tensor: Tensor) -> Tensor:
        """Process an input tensor through the LSTM layer followed by a fully connected layer and a softmax activation.

        :param input_tensor: Input tensor representing the sequence to process
        :return: Output tensor with probabilities generated by softmax
        """
        batch_size = input_tensor.size(0)

        # Initialize a hidden state for N layers
        h0 = torch.zeros(self.model_settings.n_layers, batch_size, self.model_settings.n_hidden_units,
                         device=input_tensor.device)
        c0 = torch.zeros(self.model_settings.n_layers, batch_size, self.model_settings.n_hidden_units,
                         device=input_tensor.device)

        # Process the sequence through LSTM
        _, (hidden, _) = self.lstm(input_tensor, (h0, c0))

        # Use the final hidden state for classification
        # Take the last time step from the last layer: [batch_size, hidden_size]
        final_hidden = hidden[-1]

        final_hidden = self.dropout(final_hidden)

        # Map to output classes: [batch_size, output_size]
        output = self.output_layer(final_hidden)

        # Apply log softmax for NLL loss
        return self.log_softmax(output)
